---
layout:     post
title:      "如何使用docker建造ai本地大模型"
subtitle:   "在本地建立开源大模型ollama3.2"
date:       2024-11-6
author:     "Byolio"
header-img: "img/post-bg-ollama3.jpg"
catalog: true
tags:
    - AI
    - Ollama
---
> 本文旨在建造本地开源大模型ollama3.2

注: 本项目全过程需要开启魔法用于下载ollama3.2模型和docker拉取镜像, 且需要拥有一定技术基础。

## 什么是ollama3.2
`Ollama3.2`是由`Meta`公司开发的开源多模态大型语言模型，具备处理文本和图像输入的能力。其最新版本`3.2`于`2024年9月25日`发布, 该模型系列包括1B、3B、11B、90B四个版本，参数规模从`10亿`到`900亿`不等。其中, `11B`和`90B`版本支持图像推理任务，如图像理解和视觉推理；`1B`和`3B`版本则是纯文本模型，更适用于边缘设备和移动设备的本地运行。 \
本地部署开源模型是免费的, 且使用不限次数, 可以当作平时学习工作时的助手。

## ollama3.2需要什么硬件配置
本次我要部署的是`ollama3.2`的`3B`版本, 其是最贴近笔记本硬件可以使用的版本, `Meta`虽未公布本地安装所需的硬件配置, 但根据目前模型的参数规模和常见实践可得出其对硬件配置的要求如下:  \
`3B`版本并没有`gpu`要求也可以安装`cpu`版本。但我实践后发现`cpu`版本运行速度很慢, 建议选择`gpu`版本进行安装。显存上我推荐最好在`8G`以上, 如果配置实在不行`4G`也可以, 但还是那个那个问题, 运行速度会很慢。

其他版本信息信息如下: 
1. `1B`版本配置比`3B`版本配置低, 但大体相似, 其参数过少准确率堪忧, 不推荐使用。
2. `11B`必须下载`gpu`版本, 显存上我推荐在`22G`以上, 内存要求最少在`16G`以上。
3. `90B`必须下载`gpu`版本, 显存上我推荐在`40GB`以上, 内存要求最少在`64G`以上。

## ollama3.2的性能评分
`Meta`官方对`ollama3.2 1B 和 3B`的性能与其它模型对比进行了评分, 具体如下图:

![ollama3.2](https://cdn.jsdelivr.net/gh/byolio/newtc@main/img/ollama3.2.png)

可以看到,`ollama3.2 3B`的在处理简单事情方面的评分还是不错的, 可以作为平时学习工作时的助手。
## 如何安装ollama3.2
了解完了`ollama3.2 3B`的需要的硬件配置且确定需要本地部署ai后, 那我们就可以准备开始安装`ollama3.2 3B`了。 \
我这边为了让平时使用起来更方便我决定部署带`web ui`的版本在`docker`上, 这样就可以通过浏览器访问`web ui`来使用`ollama3.2`, 代替直接在电脑本地部署通过`cmd命令行`使用。 \
我这边使用`wsl + docker`的方式实现

### 下载wsl
我决定将docker部署在win11的linux中, 所以我先安装`wsl`。(建议将win11升级到个人专业版安装wsl, 我这边使用已经升级到专业版的win11电脑演示安装过程) \
流程如下: \
打开`控制面板` -> `程序` -> `启用或关闭windows功能` -> 勾选`适用于linux的windows子系统` -> 点击`确定` -> 点击`重启`。 \
重启电脑后`wsl`就安装完成了。 \
如果不确定是否安装,  可以打开cmd输入指令`wsl --version`查看是否安装成功。

### 安装docker
我使用windows端的`docker desktop`, [点此直达下载](https://www.docker.com/products/docker-desktop/)。 选择`Download for Windows-AMD64`下载`docker desktop`, 然后进行安装, 只需要注意勾选上`Use WSL 2 instead of Hyper-V(recommended)`使用wsl即可,
接下来启动docker desktop, 可以不需要登陆直接按照接下来步骤进行部署。

### 部署web ui
部署`ollama3.2`之前我先下载`web ui`, 这边我下载的`gpu`版本的`web ui`。
打开cmd输入指令:
```bash
docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
```
这边做一下简单的介绍: \
1. docker run : 创建实例, 如果没有本地img就从`docker hub`拉取。 \
2. -d : 后台运行实例。 \
3. -p : 容器端口映射到主机。 \
4. --gpus=all : 允许容器访问所有的GPU。 \
5. -v : 卷挂载或目录挂载, 在这里是卷挂载。 \
6. --name : 容器名称。 \
7. --restart always : 容器重启策略, 这里设置为`always`表示容器容器意外停止后总是重启。 \
8. ghcr.io/open-webui/open-webui:ollama : img名称, 这里是`ollama`版本的`web ui`。
(版本一定要填, 否则拉取lastest版本)

### 启动web ui
打开浏览器在网址栏输入: `localhost:3000`后注册账号密码即可访问`web ui`。

### 下载ollama3.2 3B
点击左下角的用户头像 -> 点击`管理员面板`  \
![user](https://cdn.jsdelivr.net/gh/byolio/newtc@main/img/ollama-user.png)
然后点击`设置`->`模型`, 在下列框中
![alt text](https://cdn.jsdelivr.net/gh/byolio/newtc@main/img/ollama-img.png)
输入:
```text
llama:3b
```
然后点击旁边的下载拉取模型, 全程需要魔法, 等待下载进度条结束后重新启动`web ui`选择模型后即可使用。

## 结语
这就是我使用`docker`部署`ollama3.2 3B`的全过程, 希望对大家有所帮助。
